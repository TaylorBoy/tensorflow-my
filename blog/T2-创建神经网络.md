## 创建神经网络
</br>
### 1. 实现添加层函数
完整的「神经网络」, 包括添加神经层, 计算误差, 训练步骤, 判断是否在学习. 定义一个添加层的函数可以很容易的添加神经层.
```
import tensorflow as tf

def add_layer(inputs, in_size, out_size, activation_function=None):
	# 权重: 定义为 in_size 行 out_size 列的矩阵
	Weights = tf.Variable(tf.random_normal([in_size, out_size]))
	# 误差: 1 行 out_size 列 (输入一个维度), 推荐不为 0, 所以这里全给 0.1
	biases  = tf.Variable(tf.zeros([1, out_size]) + 0.1)
	# 我们要计算的数据模型: y = inputs * Weights + biases
	Wx_plus_b = tf.matmul(inputs, Weights) + biases
	# 是否使用激励函数?
	if activation_function is None:
		outputs = Wx_plus_b
	else:
		outputs = activation_function(Wx_plus_b)
	return outputs
```
参数:

*    inputs    : 输入数据
*    in_size   : 输入大小
*    out_size: 输出大小
*    activation_function: 激励函数, 默认不使用激励函数 (None)
</br>

### 2. 建造神经网络

1. 构建所需数据

```
import numpy as np

x_data = np.linspace(-1, 1, 300, dtype=np.float32)
#  这里加入噪点「noise」, 让数据更像真实情况
noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)
y_data = np.square(x_data) - 0.5 + noise
```
我们要构建的是: $ y = x^2 - 0.5 $


2. 定义神经网络的输入
```
# 占位符类型为 float32, None 表示输入有多少都可以, 1 表示输入只有一个特征
xs = tf.palceholder(tf.float32, [None, 1])
ys = tf.placeholder(tf.float32, [None, 1])
```

3. 定义神经层
神经层包括输入层, 隐藏层和输出层. 输入层和输出层的结构一样, 隐藏层可以自定义属性个数. 这里我们构建一个「1个 输入层, 10 个隐藏层, 1 个输出层的神经网络」.
```
# 隐藏层: 输入为我们给的 xs, 输出为要输出到输出层的 10
l1 = add_layer(xs, 1, 10, acitvation_function=tf.nn.relu)
# 输出层: 输入即为隐藏层的输出
prediction = add_layer(l1, 10, 1, activation_function=None)
```

4. 计算误差并优化
```
# 计算预测值prediction和真实值的误差, 对二者差的平方求和再取平均
loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction)),
					  reduction_indices=[1])
# 提升机器学习的准确率, 对误差 loss 进行化, 效率为 0.1 (一般小于 1)
train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
```
</br>

### 3. 让机器开始学习
注意: 上一篇提到的占位符 (placeholder) 需要填充 (feed) 值才能使用「[TensorFlow 基础](http://www.cnblogs.com/TaylorBoy/p/6749763.html "TaylorBoy")」
```
# 记得变量都要进行初始化
init_op = tf.global_variables_initializer()

# 我们让机器学习个 1000 次
with tf.Session() as sess:
	sess.run(init_op)
	for i range(1000):
		# 1000 次的学习训练, 记得占位符要填充值
		sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
		# 当然要输出来看一下学的怎么样了 (看的是误差, 50步来一次怎么样)
		if 0 == i % 50:
			print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))
	
```
</br>

***

完整代码:
```
# !/usr/bin/env python3
# -*- coding: utf-8 -*-

import tensorflow as tf
import numpy as np

def add_layer(inputs, in_size, out_size, activation_function=None):
	Weight = tf.Variable(tf.random_normal([in_size, out_size])) 
	biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) # biases not 0 is good
	Wx_plus_b = tf.matmul(inputs, Weight) + biases
	# if activation function is None or not:
	if activation_function is None:
		outputs = Wx_plus_b
	else:
		outputs = activation_function(Wx_plus_b)
	return outputs


# Create data: [-1, 1] steps: 300 newaxis: weidu--have 300 examples.
x_data = np.linspace(-1, 1, 300, dtype=np.float32)[:, np.newaxis]
noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)
y_data = np.square(x_data) - 0.5 + noise

# 利用占位符定义我们所需的神经网络的输入
xs = tf.placeholder(tf.float32, [None, 1])
ys = tf.placeholder(tf.float32, [None, 1])

# 输出层和输入层的结构是一样的；隐藏层我们可以自己假设
layer1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu) # Hidden layer
prediction = add_layer(layer1, 10, 1, activation_function=None) # Output layer

# 计算预测值prediction和真实值的误差，对二者差的平方求和再取平均
loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),
	reduction_indices=[1]))

train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

# init = tf.initialize_all_variables() 
init = tf.global_variables_initializer()

### Session Start ###
sess = tf.Session()
sess.run(init)

# 当运算要用到placeholder时，就需要feed_dict这个字典来指定输入
for i in range(1000):
	# training
	sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
	if 0 == i % 50:
		print(sess.run(loss, feed_dict={xs: x_data, ys:y_data}))

sess.close()
### Session End ###
		
```
* 输出:
0.555579
0.00811244
0.00558974
0.00454906
0.00409126
0.0037941
0.00361542
0.00350574
0.00342049
0.00335584
0.00330608
0.00326254
0.00322398
0.00318576
0.00315625
0.00313178
0.00311166
0.00309352
0.00307576
0.00305789
</br>